{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering for HybridRAG\n",
    "\n",
    "Master system prompts and reranking strategies.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Query type detection\n",
    "- Reranking instructions\n",
    "- Custom prompt creation\n",
    "- Citation handling\n",
    "- Response quality optimization\n",
    "\n",
    "**Prerequisites:** Completed notebooks 01-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from hybridrag import SYSTEM_PROMPT, SYSTEM_PROMPT_COMPACT, create_hybridrag\n",
    "from hybridrag.prompts import (\n",
    "    QueryType,\n",
    "    create_system_prompt,\n",
    "    detect_query_type,\n",
    "    select_rerank_instruction,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "rag = await create_hybridrag()\n",
    "print(\"✓ HybridRAG initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Prompts Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default system prompt (full version)\n",
    "print(\"FULL System Prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(SYSTEM_PROMPT)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Length: {len(SYSTEM_PROMPT)} characters\\n\")\n",
    "\n",
    "# Compact system prompt\n",
    "print(\"COMPACT System Prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(SYSTEM_PROMPT_COMPACT)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Length: {len(SYSTEM_PROMPT_COMPACT)} characters\\n\")\n",
    "\n",
    "print(\n",
    "    f\"Token savings: {len(SYSTEM_PROMPT) - len(SYSTEM_PROMPT_COMPACT)} characters (~{(len(SYSTEM_PROMPT) - len(SYSTEM_PROMPT_COMPACT)) // 4} tokens)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Type Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different query types\n",
    "test_queries = [\n",
    "    \"What is MongoDB Atlas?\",\n",
    "    \"Summarize the benefits of vector search\",\n",
    "    \"Show me how to create a vector index\",\n",
    "    \"Why is my query slow?\",\n",
    "    \"mongodb atlas vector search setup\",\n",
    "]\n",
    "\n",
    "print(\"Query Type Detection:\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    query_type = detect_query_type(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Type: {query_type.value}\")\n",
    "    print(f\"Pattern matched: {get_pattern_hint(query)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def get_pattern_hint(query: str) -> str:\n",
    "    \"\"\"Show which pattern triggered the detection\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    if any(\n",
    "        word in query_lower for word in [\"summary\", \"summarize\", \"explain\", \"overview\"]\n",
    "    ):\n",
    "        return \"Summary keywords\"\n",
    "    elif any(\n",
    "        word in query_lower for word in [\"how\", \"guide\", \"tutorial\", \"example\", \"show\"]\n",
    "    ):\n",
    "        return \"How-to keywords\"\n",
    "    elif any(\n",
    "        word in query_lower\n",
    "        for word in [\"error\", \"issue\", \"problem\", \"why\", \"troubleshoot\"]\n",
    "    ):\n",
    "        return \"Troubleshooting keywords\"\n",
    "    else:\n",
    "        return \"General query\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reranking Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reranking instructions for different query types\n",
    "for query_type in QueryType:\n",
    "    instruction = select_rerank_instruction(query_type)\n",
    "    print(f\"Query Type: {query_type.value}\")\n",
    "    print(f\"Reranking Instruction:\\n{instruction}\")\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Domain Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom prompts for different domains\n",
    "\n",
    "# Technical documentation domain\n",
    "tech_docs_prompt = create_system_prompt(\n",
    "    domain=\"MongoDB Atlas technical documentation\",\n",
    "    response_guidelines=[\n",
    "        \"Provide code examples when relevant\",\n",
    "        \"Link to official documentation\",\n",
    "        \"Mention version compatibility\",\n",
    "    ],\n",
    "    citation_style=\"Include document URLs and version numbers\",\n",
    ")\n",
    "\n",
    "print(\"Technical Docs Prompt:\")\n",
    "print(tech_docs_prompt)\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Customer support domain\n",
    "support_prompt = create_system_prompt(\n",
    "    domain=\"customer support for MongoDB services\",\n",
    "    response_guidelines=[\n",
    "        \"Be empathetic and patient\",\n",
    "        \"Provide step-by-step troubleshooting\",\n",
    "        \"Escalate to human if necessary\",\n",
    "    ],\n",
    "    citation_style=\"Reference knowledge base article IDs\",\n",
    ")\n",
    "\n",
    "print(\"Customer Support Prompt:\")\n",
    "print(support_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Citation Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with citation requirements\n",
    "query = \"How does vector search work in MongoDB?\"\n",
    "\n",
    "# Get answer with citations\n",
    "answer = await rag.query_with_answer(\n",
    "    query=query,\n",
    "    mode=\"hybrid\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Answer with citations:\")\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Note: The SYSTEM_PROMPT instructs the LLM to:\n",
    "# 1. Use [1], [2], [3] format for citations\n",
    "# 2. Include all sources used\n",
    "# 3. Format as: \"[X] Source: <metadata>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Quality Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain hybrid search\"\n",
    "\n",
    "# Test with different prompts\n",
    "prompts = {\n",
    "    \"Default\": SYSTEM_PROMPT,\n",
    "    \"Compact\": SYSTEM_PROMPT_COMPACT,\n",
    "    \"Technical\": tech_docs_prompt,\n",
    "}\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "for name, prompt in prompts.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Prompt Type: {name}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # Note: In practice, you'd pass the custom prompt to the LLM\n",
    "    # For this demo, we'll show the prompt characteristics\n",
    "    print(f\"Prompt length: {len(prompt)} characters\")\n",
    "    print(f\"First 200 chars: {prompt[:200]}...\")\n",
    "    print(\"\\nKey instructions:\")\n",
    "    if \"code examples\" in prompt.lower():\n",
    "        print(\"  ✓ Includes code examples\")\n",
    "    if \"citation\" in prompt.lower():\n",
    "        print(\"  ✓ Citation formatting\")\n",
    "    if \"step-by-step\" in prompt.lower():\n",
    "        print(\"  ✓ Step-by-step guidance\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adaptive Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_prompt(query: str) -> str:\n",
    "    \"\"\"Select prompt based on query characteristics\"\"\"\n",
    "    query_type = detect_query_type(query)\n",
    "\n",
    "    # Use compact for simple queries\n",
    "    if query_type == QueryType.GENERAL and len(query.split()) < 5:\n",
    "        return SYSTEM_PROMPT_COMPACT\n",
    "\n",
    "    # Use technical for how-to queries\n",
    "    elif query_type == QueryType.TOOLS:\n",
    "        return create_system_prompt(\n",
    "            domain=\"technical documentation\",\n",
    "            response_guidelines=[\"Include code examples\", \"Be precise and technical\"],\n",
    "        )\n",
    "\n",
    "    # Use empathetic for troubleshooting\n",
    "    elif query_type == QueryType.TROUBLESHOOTING:\n",
    "        return create_system_prompt(\n",
    "            domain=\"customer support\",\n",
    "            response_guidelines=[\n",
    "                \"Be helpful and patient\",\n",
    "                \"Provide step-by-step solutions\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # Default\n",
    "    else:\n",
    "        return SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "# Test adaptive prompting\n",
    "test_queries = [\n",
    "    \"What is MongoDB?\",\n",
    "    \"How do I create a vector index?\",\n",
    "    \"My query is returning no results, why?\",\n",
    "    \"Summarize the benefits of Atlas Search\",\n",
    "]\n",
    "\n",
    "print(\"Adaptive Prompt Selection:\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    selected_prompt = adaptive_prompt(query)\n",
    "    query_type = detect_query_type(query)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Type: {query_type.value}\")\n",
    "    print(\n",
    "        f\"Selected prompt: {'COMPACT' if selected_prompt == SYSTEM_PROMPT_COMPACT else 'CUSTOM' if selected_prompt != SYSTEM_PROMPT else 'DEFAULT'}\"\n",
    "    )\n",
    "    print(f\"Prompt length: {len(selected_prompt)} chars\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices\n",
    "\n",
    "### Prompt Selection:\n",
    "\n",
    "- **Default (SYSTEM_PROMPT)**: General queries, when unsure\n",
    "- **Compact (SYSTEM_PROMPT_COMPACT)**: Simple queries, token budget constraints\n",
    "- **Custom**: Domain-specific requirements\n",
    "\n",
    "### Query Type Optimization:\n",
    "\n",
    "- **GENERAL**: Standard prompt, balanced approach\n",
    "- **SUMMARY**: Emphasize conciseness and key points\n",
    "- **TOOLS**: Include code examples and technical details\n",
    "- **TROUBLESHOOTING**: Be empathetic, provide step-by-step guidance\n",
    "\n",
    "### Citation Quality:\n",
    "\n",
    "- Always include source attribution\n",
    "- Use consistent citation format ([1], [2], etc.)\n",
    "- Provide enough metadata for verification\n",
    "\n",
    "### Response Guidelines:\n",
    "\n",
    "- Keep instructions clear and actionable\n",
    "- Avoid contradictory guidelines\n",
    "- Test prompts with real queries\n",
    "- Iterate based on user feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- `05_performance_tuning.ipynb` - Production optimization\n",
    "- **Further reading:**\n",
    "  - Prompt Engineering Best Practices\n",
    "  - LLM Response Quality Evaluation\n",
    "  - Citation and Attribution Standards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
