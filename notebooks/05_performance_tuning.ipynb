{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Tuning for Production\n",
    "\n",
    "Optimize HybridRAG for production deployment.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Search parameter optimization\n",
    "- Caching strategies\n",
    "- Batch processing\n",
    "- Performance benchmarking\n",
    "- Cost optimization\n",
    "\n",
    "**Prerequisites:** Completed notebooks 01-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List\n",
    "from hybridrag import create_hybridrag\n",
    "from hybridrag.config import get_settings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "rag = await create_hybridrag()\n",
    "print(\"âœ“ HybridRAG initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def measure_query_latency(query: str, mode: str, top_k: int) -> float:\n",
    "    \"\"\"Measure single query latency\"\"\"\n",
    "    start = time.time()\n",
    "    results = await rag.query(query=query, mode=mode, top_k=top_k)\n",
    "    latency = time.time() - start\n",
    "    return latency\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is MongoDB Atlas?\",\n",
    "    \"How does vector search work?\",\n",
    "    \"mongodb hybrid search configuration\",\n",
    "]\n",
    "\n",
    "print(\"Baseline Latency Measurements:\\n\")\n",
    "\n",
    "for mode in [\"vector\", \"keyword\", \"hybrid\"]:\n",
    "    latencies = []\n",
    "    for query in test_queries:\n",
    "        latency = await measure_query_latency(query, mode, top_k=10)\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    print(f\"{mode.upper()} Mode:\")\n",
    "    print(f\"  Average: {avg_latency*1000:.2f}ms\")\n",
    "    print(f\"  Min: {min(latencies)*1000:.2f}ms\")\n",
    "    print(f\"  Max: {max(latencies)*1000:.2f}ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different top_k values\n",
    "query = \"mongodb atlas vector search\"\n",
    "top_k_values = [5, 10, 20, 50]\n",
    "\n",
    "print(\"Impact of top_k on latency:\\n\")\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    latency = await measure_query_latency(query, \"hybrid\", top_k)\n",
    "    print(f\"top_k={top_k:3d}: {latency*1000:6.2f}ms (numCandidates={top_k*20})\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"- top_k=10: Good balance for most use cases\")\n",
    "print(\"- top_k=5: Faster, acceptable for quick searches\")\n",
    "print(\"- top_k=20+: Slower, use only when higher recall needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sequential vs batch processing\n",
    "queries = [\n",
    "    \"mongodb vector search\",\n",
    "    \"atlas search features\",\n",
    "    \"hybrid search setup\",\n",
    "    \"knowledge graph mongodb\",\n",
    "    \"rag system architecture\",\n",
    "]\n",
    "\n",
    "# Sequential processing\n",
    "start = time.time()\n",
    "sequential_results = []\n",
    "for query in queries:\n",
    "    results = await rag.query(query=query, mode=\"hybrid\", top_k=5)\n",
    "    sequential_results.append(results)\n",
    "sequential_time = time.time() - start\n",
    "\n",
    "# Batch processing (parallel)\n",
    "start = time.time()\n",
    "batch_tasks = [rag.query(query=q, mode=\"hybrid\", top_k=5) for q in queries]\n",
    "batch_results = await asyncio.gather(*batch_tasks)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential: {sequential_time*1000:.2f}ms ({sequential_time*1000/len(queries):.2f}ms per query)\")\n",
    "print(f\"Batch: {batch_time*1000:.2f}ms ({batch_time*1000/len(queries):.2f}ms per query)\")\n",
    "print(f\"Speedup: {sequential_time/batch_time:.2f}x\\n\")\n",
    "\n",
    "print(\"Recommendation:\")\n",
    "print(\"- Use batch processing for multiple queries\")\n",
    "print(\"- Significant speedup for I/O-bound operations\")\n",
    "print(\"- Monitor MongoDB connection limits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Caching Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple cache implementation\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class QueryCache:\n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def get_key(self, query: str, mode: str, top_k: int) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        content = f\"{query}:{mode}:{top_k}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, query: str, mode: str, top_k: int):\n",
    "        key = self.get_key(query, mode, top_k)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, query: str, mode: str, top_k: int, results):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Simple FIFO eviction\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        key = self.get_key(query, mode, top_k)\n",
    "        self.cache[key] = results\n",
    "\n",
    "cache = QueryCache(max_size=100)\n",
    "\n",
    "async def cached_query(query: str, mode: str = \"hybrid\", top_k: int = 10):\n",
    "    \"\"\"Query with caching\"\"\"\n",
    "    cached = cache.get(query, mode, top_k)\n",
    "    if cached:\n",
    "        return cached, True  # Cache hit\n",
    "    \n",
    "    results = await rag.query(query=query, mode=mode, top_k=top_k)\n",
    "    cache.set(query, mode, top_k, results)\n",
    "    return results, False  # Cache miss\n",
    "\n",
    "# Test caching\n",
    "query = \"mongodb vector search atlas\"\n",
    "\n",
    "# First query (cache miss)\n",
    "start = time.time()\n",
    "results1, hit1 = await cached_query(query)\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Second query (cache hit)\n",
    "start = time.time()\n",
    "results2, hit2 = await cached_query(query)\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"First query (miss): {time1*1000:.2f}ms\")\n",
    "print(f\"Second query (hit): {time2*1000:.2f}ms\")\n",
    "print(f\"Speedup: {time1/time2:.1f}x\\n\")\n",
    "\n",
    "print(\"Caching Best Practices:\")\n",
    "print(\"- Cache frequently asked queries\")\n",
    "print(\"- Set appropriate TTL (e.g., 5-15 minutes)\")\n",
    "print(\"- Use Redis/Memcached for distributed caching\")\n",
    "print(\"- Monitor cache hit rate (target: >70%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Cost Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embedding costs\n",
    "settings = get_settings()\n",
    "\n",
    "# Voyage AI pricing (example)\n",
    "VOYAGE_PRICING = {\n",
    "    \"voyage-3\": 0.06 / 1_000_000,  # $0.06 per 1M tokens\n",
    "    \"voyage-3-large\": 0.13 / 1_000_000,  # $0.13 per 1M tokens\n",
    "}\n",
    "\n",
    "def estimate_embedding_cost(texts: List[str], model: str = \"voyage-3\") -> dict:\n",
    "    \"\"\"Estimate embedding costs\"\"\"\n",
    "    total_tokens = sum(len(text.split()) * 1.3 for text in texts)  # ~1.3 tokens per word\n",
    "    cost_per_token = VOYAGE_PRICING.get(model, 0.06 / 1_000_000)\n",
    "    total_cost = total_tokens * cost_per_token\n",
    "    \n",
    "    return {\n",
    "        \"total_texts\": len(texts),\n",
    "        \"total_tokens\": int(total_tokens),\n",
    "        \"cost\": total_cost,\n",
    "        \"cost_per_text\": total_cost / len(texts),\n",
    "    }\n",
    "\n",
    "# Example: 1000 documents\n",
    "sample_docs = [\"This is a sample document about MongoDB Atlas.\" * 10] * 1000\n",
    "\n",
    "for model in [\"voyage-3\", \"voyage-3-large\"]:\n",
    "    cost_estimate = estimate_embedding_cost(sample_docs, model)\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Documents: {cost_estimate['total_texts']:,}\")\n",
    "    print(f\"  Tokens: {cost_estimate['total_tokens']:,}\")\n",
    "    print(f\"  Total cost: ${cost_estimate['cost']:.4f}\")\n",
    "    print(f\"  Cost per doc: ${cost_estimate['cost_per_text']:.6f}\\n\")\n",
    "\n",
    "print(\"Cost Optimization Tips:\")\n",
    "print(\"- Use voyage-3 (smaller model) when possible\")\n",
    "print(\"- Batch embeddings (up to 128 texts per API call)\")\n",
    "print(\"- Cache embeddings - don't re-embed unchanged content\")\n",
    "print(\"- Chunk documents efficiently (512-1024 tokens per chunk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MongoDB Index Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MongoDB indexes\n",
    "from pymongo import MongoClient\n",
    "\n",
    "settings = get_settings()\n",
    "client = MongoClient(settings.MONGODB_URI)\n",
    "db = client[settings.MONGODB_DATABASE]\n",
    "collection = db[settings.COLLECTION_NAME]\n",
    "\n",
    "print(\"Current Indexes:\\n\")\n",
    "for idx in collection.list_indexes():\n",
    "    print(f\"Name: {idx['name']}\")\n",
    "    print(f\"Keys: {idx['key']}\")\n",
    "    if 'type' in idx:\n",
    "        print(f\"Type: {idx['type']}\")\n",
    "    print()\n",
    "\n",
    "print(\"Index Optimization Tips:\")\n",
    "print(\"\\nVector Search Index:\")\n",
    "print(\"- Use HNSW for better performance\")\n",
    "print(\"- Tune numDimensions to match embeddings (1024 for voyage-3)\")\n",
    "print(\"- Set similarity to 'cosine' for normalized vectors\")\n",
    "\n",
    "print(\"\\nAtlas Search Index:\")\n",
    "print(\"- Index only fields you actually search\")\n",
    "print(\"- Use analyzers appropriate for your content\")\n",
    "print(\"- Enable autocomplete only if needed\")\n",
    "\n",
    "print(\"\\nCompound Indexes:\")\n",
    "print(\"- Create indexes for common filter fields\")\n",
    "print(\"- Order matters: most selective field first\")\n",
    "print(\"- Monitor index usage with explain()\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.query_times = []\n",
    "        self.query_counts = {\"vector\": 0, \"keyword\": 0, \"hybrid\": 0}\n",
    "    \n",
    "    def record_query(self, mode: str, latency: float):\n",
    "        self.query_times.append(latency)\n",
    "        self.query_counts[mode] += 1\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        if not self.query_times:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.query_times),\n",
    "            \"avg_latency_ms\": statistics.mean(self.query_times) * 1000,\n",
    "            \"median_latency_ms\": statistics.median(self.query_times) * 1000,\n",
    "            \"p95_latency_ms\": statistics.quantiles(self.query_times, n=20)[18] * 1000 if len(self.query_times) >= 20 else max(self.query_times) * 1000,\n",
    "            \"max_latency_ms\": max(self.query_times) * 1000,\n",
    "            \"mode_distribution\": self.query_counts,\n",
    "        }\n",
    "\n",
    "# Simulate production load\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "test_queries = [\n",
    "    (\"mongodb atlas\", \"hybrid\"),\n",
    "    (\"vector search\", \"vector\"),\n",
    "    (\"full text search\", \"keyword\"),\n",
    "] * 10\n",
    "\n",
    "print(\"Simulating production load...\\n\")\n",
    "\n",
    "for query, mode in test_queries:\n",
    "    latency = await measure_query_latency(query, mode, top_k=10)\n",
    "    monitor.record_query(mode, latency)\n",
    "\n",
    "stats = monitor.get_stats()\n",
    "\n",
    "print(\"Performance Statistics:\")\n",
    "print(f\"\\nTotal queries: {stats['total_queries']}\")\n",
    "print(f\"Average latency: {stats['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"Median latency: {stats['median_latency_ms']:.2f}ms\")\n",
    "print(f\"P95 latency: {stats['p95_latency_ms']:.2f}ms\")\n",
    "print(f\"Max latency: {stats['max_latency_ms']:.2f}ms\")\n",
    "\n",
    "print(f\"\\nMode distribution:\")\n",
    "for mode, count in stats['mode_distribution'].items():\n",
    "    print(f\"  {mode}: {count} ({count/stats['total_queries']*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTarget SLAs:\")\n",
    "print(\"  - P50 (median): <200ms\")\n",
    "  print(\"  - P95: <500ms\")\n",
    "print(\"  - P99: <1000ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Checklist\n",
    "\n",
    "### Search Optimization:\n",
    "- [ ] Set appropriate `top_k` (default: 10)\n",
    "- [ ] Configure `numCandidates` (automatic: top_k * 20)\n",
    "- [ ] Enable `scoreDetails` for debugging\n",
    "- [ ] Tune vector/text weights based on query type\n",
    "\n",
    "### Caching:\n",
    "- [ ] Implement query result caching\n",
    "- [ ] Cache embeddings for unchanged content\n",
    "- [ ] Set appropriate TTL (5-15 minutes)\n",
    "- [ ] Monitor cache hit rate (target: >70%)\n",
    "\n",
    "### MongoDB:\n",
    "- [ ] Create appropriate indexes\n",
    "- [ ] Monitor index usage with explain()\n",
    "- [ ] Use connection pooling\n",
    "- [ ] Enable read preference (secondary for reads)\n",
    "\n",
    "### Cost Optimization:\n",
    "- [ ] Use smaller embedding model when possible\n",
    "- [ ] Batch API calls\n",
    "- [ ] Implement request deduplication\n",
    "- [ ] Monitor API usage\n",
    "\n",
    "### Monitoring:\n",
    "- [ ] Track query latency (P50, P95, P99)\n",
    "- [ ] Monitor error rates\n",
    "- [ ] Set up alerts for SLA violations\n",
    "- [ ] Log slow queries (>1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've learned:\n",
    "- Performance measurement and benchmarking\n",
    "- Parameter optimization strategies\n",
    "- Caching implementation\n",
    "- Cost optimization techniques\n",
    "- Production monitoring\n",
    "\n",
    "**Next steps:**\n",
    "- Deploy to production\n",
    "- Monitor real-world performance\n",
    "- Iterate based on metrics\n",
    "- Scale as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
